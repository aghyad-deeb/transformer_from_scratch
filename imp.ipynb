{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from beartype import beartype as typechecker\n",
    "import numpy as np\n",
    "from jaxtyping import Float, jaxtyped\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional embedding as defined by Vaswani et al., 2017.\n",
    "class PosEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=4, n_tokens=8, debug=False):\n",
    "        super().__init__()\n",
    "        self.debug = debug\n",
    "        self.d_model = d_model\n",
    "        self.n_tokens = n_tokens\n",
    "\n",
    "        posEmbedLst = []\n",
    "        for pos in range(n_tokens):\n",
    "            row = []\n",
    "            for dim in range(d_model):\n",
    "                if dim % 2 == 0:\n",
    "                    row += [self.sin(pos, dim)]\n",
    "                else:\n",
    "                    row += [self.cos(pos, dim)]\n",
    "            \n",
    "            posEmbedLst.append(row.copy())\n",
    "\n",
    "        self.posEmbed : Float[Tensor, \"n_tokens d_model\"] = (\n",
    "            torch.tensor(posEmbedLst)\n",
    "        )\n",
    "        if debug:\n",
    "            print(f\"{self.posEmbed=}\")\n",
    "\n",
    "    def sin(self, pos, evenDim):\n",
    "        return torch.sin(\n",
    "            torch.tensor(pos/self.n_tokens**(evenDim/self.d_model))\n",
    "        )\n",
    "    \n",
    "    def cos(self, pos, oddDim):\n",
    "        return torch.cos(\n",
    "            torch.tensor(pos/self.n_tokens**(oddDim/self.d_model))\n",
    "        )\n",
    "    \n",
    "    @jaxtyped(typechecker=typechecker)\n",
    "    def forward(\n",
    "        self, input: Float[Tensor, \"n_tokens d_model\"]\n",
    "    ) -> Float[Tensor, \"n_tokens d_model\"]:\n",
    "        # I actually dont need and input!\n",
    "        r = input + self.posEmbed\n",
    "        if self.debug:\n",
    "            print(f\"{r=}\")\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 2.0000, 1.0000, 2.0000],\n",
       "        [1.8415, 1.8284, 1.3462, 1.9780],\n",
       "        [1.9093, 1.3724, 1.6496, 1.9129],\n",
       "        [1.1411, 0.7886, 1.8727, 1.8076],\n",
       "        [0.2432, 0.2774, 1.9878, 1.6668],\n",
       "        [0.0411, 0.0142, 1.9807, 1.4966],\n",
       "        [0.7206, 0.0894, 1.8523, 1.3045],\n",
       "        [1.6570, 0.4772, 1.6184, 1.0991]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones((8, 4))\n",
    "PosEmbed().forward(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape: n_tokens x d_model\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=4, n_tokens=8, n_head=2, debug=False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_tokens = n_tokens\n",
    "        self.n_head = n_head\n",
    "        self.debug = debug\n",
    "\n",
    "        self.d_head = d_head = d_model // n_head\n",
    "        self.wQ: Float[Tensor, \"d_model d_head\"] = (\n",
    "            torch.rand((d_model, d_head))\n",
    "        )\n",
    "        self.wK: Float[Tensor, \"d_model d_head\"] = (\n",
    "            torch.rand((d_model, d_head))\n",
    "        )\n",
    "        self.wV: Float[Tensor, \"d_model d_head\"] = (\n",
    "            torch.rand((d_model, d_head))\n",
    "        )\n",
    "\n",
    "    @jaxtyped(typechecker=typechecker)\n",
    "    def forward(\n",
    "        self, input: Float[Tensor, \"n_tokens d_model\"]\n",
    "    ) -> Float[Tensor, \"n_tokens d_head\"]:\n",
    "        Q: Float[Tensor, \"n_tokens d_head\"] = input @ self.wQ\n",
    "        K: Float[Tensor, \"n_tokens d_head\"] = input @ self.wK\n",
    "        A: Float[Tensor, \"n_tokens n_token\"] = Q @ K.transpose(dim0=-2, dim1=-1)\n",
    "        scores: Float[Tensor, \"n_tokens n_token\"] = (\n",
    "            torch.softmax(A / self.d_model**(1/2), dim=-1)\n",
    "        )\n",
    "        V: Float[Tensor, \"n_tokens d_head\"] = input @ self.wV\n",
    "        output: Float[Tensor, \"n_tokens d_head\"] = scores @ V\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4602, 1.4841],\n",
       "        [2.4602, 1.4841],\n",
       "        [2.4602, 1.4841],\n",
       "        [2.4602, 1.4841],\n",
       "        [2.4602, 1.4841],\n",
       "        [2.4602, 1.4841],\n",
       "        [2.4602, 1.4841],\n",
       "        [2.4602, 1.4841]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones((8, 4))\n",
    "Head().forward(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=4, n_tokens=8, mlp_factor=4, debug=False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_tokens = n_tokens\n",
    "        self.mlp_factor = mlp_factor\n",
    "        self.debug = debug\n",
    "\n",
    "        self.encode: Float[Tensor, \"d_model d_model*mlp_factor\"] = (\n",
    "            torch.rand((d_model, d_model*mlp_factor))\n",
    "        )\n",
    "        self.decode: Float[Tensor, \"d_model*mlp_factor d_model\"] = (\n",
    "            torch.rand((d_model*mlp_factor, d_model))\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self, input: Float[Tensor, \"n_tokens d_model\"]\n",
    "    ) -> Float[Tensor, \"n_tokens d_model\"]:\n",
    "        output: Float[Tensor, \"n_tokens d_model\"] = (\n",
    "            input @ self.encode @ self.decode \n",
    "        )\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18.2701, 15.6138, 15.3038, 14.6008],\n",
       "        [18.2701, 15.6138, 15.3038, 14.6008],\n",
       "        [18.2701, 15.6138, 15.3038, 14.6008],\n",
       "        [18.2701, 15.6138, 15.3038, 14.6008],\n",
       "        [18.2701, 15.6138, 15.3038, 14.6008],\n",
       "        [18.2701, 15.6138, 15.3038, 14.6008],\n",
       "        [18.2701, 15.6138, 15.3038, 14.6008],\n",
       "        [18.2701, 15.6138, 15.3038, 14.6008]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FeedForward().forward(torch.ones((8, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=4, n_tokens=8, debug=False):\n",
    "        self.d_model = d_model\n",
    "        self.n_tokens = n_tokens\n",
    "        self.debug = debug\n",
    "\n",
    "    def forward(\n",
    "        self, input: Float[Tensor, \"n_tokens d_model\"]\n",
    "    ) -> Float[Tensor, \"n_tokens d_model\"]:\n",
    "        output = (input - input.mean(dim=-1))/input.std(dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=4, n_tokens=8, n_head=2, mlp_factor=4, debug=False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_tokens = n_tokens\n",
    "        self.n_head = n_head\n",
    "        self.mlp_factor = mlp_factor\n",
    "        self.debug = debug\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            Head(\n",
    "                d_model=d_model, n_tokens=n_tokens, n_head=n_head, debug=debug\n",
    "            ) for _ in range(n_head)\n",
    "        ])\n",
    "        self.wO: Float[Tensor, \"d_model d_model\"] = torch.rand((\n",
    "            d_model, d_model                                                         \n",
    "        ))\n",
    "        self.ffw = FeedForward(\n",
    "            d_model=d_model, n_tokens=n_tokens,\n",
    "            mlp_factor=mlp_factor, debug=debug\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, input: Float[Tensor, \"n_tokens d_model\"]\n",
    "    ) -> Float[Tensor, \"n_tokens d_model\"]:\n",
    "        outsLst: list[Float[Tensor, \"n_token d_model/n_head\"]] = [\n",
    "            h(input.clone()).tolist() for h in self.heads\n",
    "        ]\n",
    "        outs: Float[Tensor, \"n_head n_tokens d_model/n_head\"] = torch.tensor(\n",
    "            outsLst\n",
    "        )\n",
    "        outsCat: Float[Tensor, \"n_tokens d_model\"] = outs.view(\n",
    "            -1, self.d_model\n",
    "        )\n",
    "        outsProj: Float[Tensor, \"n_tokens d_model\"] = outsCat @ self.wO\n",
    "        output: Float[Tensor, \"n_tokens d_model\"] = self.ffw(outsProj)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 93.8969,  97.5771, 102.0062,  83.3961],\n",
       "        [ 93.8969,  97.5771, 102.0062,  83.3961],\n",
       "        [ 93.8969,  97.5771, 102.0062,  83.3961],\n",
       "        [ 93.8969,  97.5771, 102.0062,  83.3961],\n",
       "        [ 78.6800,  81.8623,  85.5446,  69.9256],\n",
       "        [ 78.6800,  81.8623,  85.5446,  69.9256],\n",
       "        [ 78.6800,  81.8623,  85.5446,  69.9256],\n",
       "        [ 78.6800,  81.8623,  85.5446,  69.9256]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HiddenLayer().forward(torch.ones((8, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, num_layers=5, d_model=4, n_vocab=16,\n",
    "        n_tokens=8, n_head=2, mlp_factor=4, debug=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_tokens = n_tokens\n",
    "        self.n_head = n_head\n",
    "        self.mlp_factor = mlp_factor\n",
    "        self.debug = debug\n",
    "\n",
    "        self.embed: Float[Tensor, \"n_vocab d_model\"] = (\n",
    "            torch.rand((n_vocab, d_model))\n",
    "        )\n",
    "        self.posEmbed = PosEmbed(\n",
    "            d_model=d_model, n_tokens=n_tokens, debug=debug\n",
    "        )\n",
    "        self.hiddenLayers = nn.ModuleList([\n",
    "            HiddenLayer(\n",
    "                d_model=d_model, n_tokens=n_tokens, n_head=n_head,\n",
    "                mlp_factor=mlp_factor, debug=debug\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
