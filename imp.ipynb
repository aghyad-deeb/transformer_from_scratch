{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from beartype import beartype as typechecker\n",
    "import numpy as np\n",
    "from jaxtyping import UInt, Float, jaxtyped\n",
    "from torch import Tensor\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed():\n",
    "    torch.manual_seed(4)\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional embedding as defined by Vaswani et al., 2017.\n",
    "class PosEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=4, n_tokens=8, debug=False):\n",
    "        super().__init__()\n",
    "        self.debug = debug\n",
    "        self.d_model = d_model\n",
    "        self.n_tokens = n_tokens\n",
    "\n",
    "        posEmbedLst = []\n",
    "        for pos in range(n_tokens):\n",
    "            row = []\n",
    "            for dim in range(d_model):\n",
    "                if dim % 2 == 0:\n",
    "                    row += [self.sin(pos, dim)]\n",
    "                else:\n",
    "                    row += [self.cos(pos, dim)]\n",
    "            \n",
    "            posEmbedLst.append(row.copy())\n",
    "\n",
    "        self.posEmbed : Float[Tensor, \"n_tokens d_model\"] = nn.Parameter((\n",
    "            torch.tensor(posEmbedLst)\n",
    "        ))\n",
    "        if debug:\n",
    "            print(f\"{self.posEmbed=}\")\n",
    "\n",
    "    def sin(self, pos, evenDim):\n",
    "        return torch.sin(\n",
    "            torch.tensor(pos/self.n_tokens**(evenDim/self.d_model))\n",
    "        )\n",
    "    \n",
    "    def cos(self, pos, oddDim):\n",
    "        return torch.cos(\n",
    "            torch.tensor(pos/self.n_tokens**(oddDim/self.d_model))\n",
    "        )\n",
    "    \n",
    "    @jaxtyped(typechecker=typechecker)\n",
    "    def forward(\n",
    "        self, input: Float[Tensor, \"*batch_size n_tokens d_model\"]\n",
    "    ) -> Float[Tensor, \"*batch_size n_tokens d_model\"]:\n",
    "        # I actually dont need and input!\n",
    "        r = input + self.posEmbed\n",
    "        if self.debug:\n",
    "            display(f\"{r=}\")\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 2.0000, 1.0000, 2.0000],\n",
       "        [1.8415, 1.8284, 1.3462, 1.9780],\n",
       "        [1.9093, 1.3724, 1.6496, 1.9129],\n",
       "        [1.1411, 0.7886, 1.8727, 1.8076],\n",
       "        [0.2432, 0.2774, 1.9878, 1.6668],\n",
       "        [0.0411, 0.0142, 1.9807, 1.4966],\n",
       "        [0.7206, 0.0894, 1.8523, 1.3045],\n",
       "        [1.6570, 0.4772, 1.6184, 1.0991]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones((8, 4))\n",
    "PosEmbed().forward(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape: n_tokens x d_model\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=4, n_tokens=8, n_head=2, debug=False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_tokens = n_tokens\n",
    "        self.n_head = n_head\n",
    "        self.debug = debug\n",
    "\n",
    "        self.d_head = d_head = d_model // n_head\n",
    "        self.wQ: Float[Tensor, \"d_model d_head\"] = nn.Parameter(\n",
    "            torch.rand((d_model, d_head))\n",
    "        )\n",
    "        self.wK: Float[Tensor, \"d_model d_head\"] = nn.Parameter(\n",
    "            torch.rand((d_model, d_head))\n",
    "        )\n",
    "        self.wV: Float[Tensor, \"d_model d_head\"] = nn.Parameter(\n",
    "            torch.rand((d_model, d_head))\n",
    "        )\n",
    "\n",
    "    @jaxtyped(typechecker=typechecker)\n",
    "    def forward(\n",
    "        self, input: Float[Tensor, \"*batch_size n_tokens d_model\"]\n",
    "    ) -> Float[Tensor, \"*batch_size n_tokens d_head\"]:\n",
    "        Q: Float[Tensor, \"*batch_size n_tokens d_head\"] = input @ self.wQ\n",
    "        K: Float[Tensor, \"*batch_size n_tokens d_head\"] = input @ self.wK\n",
    "        A: Float[Tensor, \"*batch_size n_tokens n_token\"] = (\n",
    "            Q @ K.transpose(dim0=-2, dim1=-1)\n",
    "        )\n",
    "        if self.debug:\n",
    "            display(A)\n",
    "        masked: Float[Tensor, \"*batch_size n_tokens n_tokens\"] = (\n",
    "            A + (torch.ones_like(A) * float(\"-inf\")).triu(diagonal=1)\n",
    "        )\n",
    "        if self.debug:\n",
    "            display(masked)\n",
    "        scores: Float[Tensor, \"*batch_size n_tokens n_token\"] = (\n",
    "            torch.softmax(masked / self.d_model**(1/2), dim=-1)\n",
    "        )\n",
    "        if self.debug:\n",
    "            display(scores)\n",
    "        V: Float[Tensor, \"*batch_size n_tokens d_head\"] = input @ self.wV\n",
    "        output: Float[Tensor, \"*batch_size n_tokens d_head\"] = scores @ V\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "a = torch.ones((1, 8, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=4, n_tokens=8, mlp_factor=4, debug=False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_tokens = n_tokens\n",
    "        self.mlp_factor = mlp_factor\n",
    "        self.debug = debug\n",
    "\n",
    "        self.encode: Float[Tensor, \"d_model d_model*mlp_factor\"] = nn.Parameter(\n",
    "            torch.rand((d_model, d_model*mlp_factor))\n",
    "        )\n",
    "        self.decode: Float[Tensor, \"d_model*mlp_factor d_model\"] = nn.Parameter(\n",
    "            torch.rand((d_model*mlp_factor, d_model))\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self, input: Float[Tensor, \"*batch_size n_tokens d_model\"]\n",
    "    ) -> Float[Tensor, \"*batch_size n_tokens d_model\"]:\n",
    "        output: Float[Tensor, \"*batch_size n_tokens d_model\"] = (\n",
    "            input @ self.encode @ self.decode \n",
    "        )\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15.1447, 19.8279, 22.9614, 16.4457],\n",
       "        [15.1447, 19.8279, 22.9614, 16.4457],\n",
       "        [15.1447, 19.8279, 22.9614, 16.4457],\n",
       "        [15.1447, 19.8279, 22.9614, 16.4457],\n",
       "        [15.1447, 19.8279, 22.9614, 16.4457],\n",
       "        [15.1447, 19.8279, 22.9614, 16.4457],\n",
       "        [15.1447, 19.8279, 22.9614, 16.4457],\n",
       "        [15.1447, 19.8279, 22.9614, 16.4457]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed()\n",
    "FeedForward().forward(torch.ones((8, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=4, n_tokens=8, debug=False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_tokens = n_tokens\n",
    "        self.debug = debug\n",
    "\n",
    "    def forward(\n",
    "        self, input: Float[Tensor, \"*batch_size n_tokens d_model\"]\n",
    "    ) -> Float[Tensor, \"*batch_size n_tokens d_model\"]:\n",
    "        output = (\n",
    "            (input - input.mean(dim=-1, keepdim=True))\n",
    "            /input.std(dim=-1, keepdim=True)\n",
    "        )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.4901e-08,  0.0000e+00,  4.4703e-08,  6.7055e-08, -1.4901e-08,\n",
       "         -1.1176e-08,  1.0431e-07,  1.1176e-07]),\n",
       " tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed()\n",
    "ln_ed = LayerNorm().forward(torch.rand((8, 4)))\n",
    "ln_ed.mean(-1), ln_ed.std(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=4, n_tokens=8, n_head=2, mlp_factor=4, debug=False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_tokens = n_tokens\n",
    "        self.n_head = n_head\n",
    "        self.mlp_factor = mlp_factor\n",
    "        self.debug = debug\n",
    "\n",
    "        self.ln = LayerNorm(\n",
    "            d_model=self.d_model, n_tokens=self.n_tokens\n",
    "        )\n",
    "        self.heads = nn.ModuleList([\n",
    "            Head(\n",
    "                d_model=d_model, n_tokens=n_tokens, n_head=n_head, debug=debug\n",
    "            ) for _ in range(n_head)\n",
    "        ])\n",
    "        self.wO: Float[Tensor, \"d_model d_model\"] = nn.Parameter(torch.rand((\n",
    "            d_model, d_model                                                         \n",
    "        )))\n",
    "        self.ffw = FeedForward(\n",
    "            d_model=d_model, n_tokens=n_tokens,\n",
    "            mlp_factor=mlp_factor, debug=debug\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, input: Float[Tensor, \"*batch_size n_tokens d_model\"]\n",
    "    ) -> Float[Tensor, \"*batch_size n_tokens d_model\"]:\n",
    "        normInput = self.ln(input.clone())\n",
    "        outsLst: list[Float[Tensor, \"*batch_size n_token d_model/n_head\"]] = [\n",
    "            h(normInput.clone()).tolist() for h in self.heads\n",
    "        ]\n",
    "        outs: Float[Tensor, \"n_head *batch_size n_tokens d_model/n_head\"] = (\n",
    "            torch.tensor(outsLst)\n",
    "        )\n",
    "        for i in range(0, len(outs.shape) - 3):\n",
    "            outs = outs.transpose(i, i+1)\n",
    "        outsTrans: Float[Tensor, \"*batch_size n_tokens n_head d_model/n_head\"] = (\n",
    "            outs.transpose(-3, -2).contiguous()\n",
    "        )\n",
    "        outsCat: Float[Tensor, \"*batch_size n_tokens d_model\"] = outsTrans.view(\n",
    "            tuple([*outsTrans.shape[:-2], self.d_model])\n",
    "        )\n",
    "        outsProj: Float[Tensor, \"*batch_size n_tokens d_model\"] = outsCat @ self.wO\n",
    "        outsSum: Float[Tensor, \"*batch_size n_tokens d_model\"] = outsProj + input.clone()\n",
    "        outsNorm: Float[Tensor, \"*batch_size n_tokens d_model\"] = self.ln(outsSum)\n",
    "        output: Float[Tensor, \"*batch_size n_tokens d_model\"] = self.ffw(outsNorm)\n",
    "        return output + input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wO',\n",
       " 'heads.0.wQ',\n",
       " 'heads.0.wK',\n",
       " 'heads.0.wV',\n",
       " 'heads.1.wQ',\n",
       " 'heads.1.wK',\n",
       " 'heads.1.wV',\n",
       " 'ffw.encode',\n",
       " 'ffw.decode']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed()\n",
    "[name for name, _  in list(HiddenLayer().named_parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, num_layers=5, d_model=4, n_vocab=16,\n",
    "        n_tokens=8, n_head=2, mlp_factor=4, debug=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_tokens = n_tokens\n",
    "        self.n_head = n_head\n",
    "        self.mlp_factor = mlp_factor\n",
    "        self.debug = debug\n",
    "\n",
    "        self.embed: Float[Tensor, \"n_vocab d_model\"] = nn.Parameter(\n",
    "            torch.rand((n_vocab, d_model))\n",
    "        )\n",
    "        # self.embed[-1, :] = torch.tensor([float(\"-inf\")])\n",
    "        self.posEmbed = PosEmbed(\n",
    "            d_model=d_model, n_tokens=n_tokens, debug=debug\n",
    "        )\n",
    "        self.hiddenLayers = nn.ModuleList([\n",
    "            HiddenLayer(\n",
    "                d_model=d_model, n_tokens=n_tokens, n_head=n_head,\n",
    "                mlp_factor=mlp_factor, debug=debug\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.unembed: Float[Tensor, \"d_model n_vocab\"] = nn.Parameter(\n",
    "            torch.rand((d_model, n_vocab))\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self, input: Float[Tensor, \"*batch_size n_tokens n_vocab\"]\n",
    "    ) -> Float[Tensor, \"*batch_size n_tokens n_vocab\"]:\n",
    "        mask: Float[Tensor, \"n_vocab d_model\"] = torch.ones_like(self.embed)\n",
    "        mask[-1, :] = float(\"0\")\n",
    "        maskedEmbed: Float[Tensor, \"n_vocab d_model\"] = self.embed *  mask\n",
    "        emb: Float[Tensor, \"*batch_size n_tokens d_model\"] = self.posEmbed(\n",
    "            input @ maskedEmbed\n",
    "        )\n",
    "        out: Float[Tensor, \"*batch_size n_tokens d_model\"] = emb\n",
    "        for hl in self.hiddenLayers:\n",
    "            out = hl(out)\n",
    "        logits: Float[Tensor, \"*batch_size n_tokens n_vocab\"] = out @ self.unembed\n",
    "        return logits\n",
    "    \n",
    "    def inference(\n",
    "        self, lst: list[int]\n",
    "    ):\n",
    "        tsr = torch.tensor([lst[i] if i < len(lst) else 15 for i in range(8)])\n",
    "        inp = F.one_hot(\n",
    "            tsr, num_classes=self.n_vocab\n",
    "        ).to(dtype=torch.float)\n",
    "        return self.forward(inp).argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9,  0,  7,  9,  9,  9,  9, 11])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed()\n",
    "# Transformer().forward(torch.zeros((8, 16)))\n",
    "# Transformer().forward(torch.rand((2, 8, 16)))\n",
    "Transformer().inference([1, 3, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(name, t.shape) for name, t in Transformer().named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import wandb\n",
    "\n",
    "class TransformerTrainer():\n",
    "    \n",
    "    def __init__(\n",
    "        self, model, optim_choice=Adam, epochs=1, lr=1e-7,\n",
    "        batch_size=4, debug=False\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optim_choice = optim_choice\n",
    "        self.epochs = epochs\n",
    "        self.lr=lr\n",
    "        self.batch_size = batch_size\n",
    "        self.debug = debug\n",
    "    \n",
    "    def getLoss(\n",
    "        self, batch: Float[Tensor, \"*batch_size n_tokens\"]\n",
    "    ):\n",
    "        inp = F.one_hot(\n",
    "            batch, num_classes=self.model.n_vocab,\n",
    "        ).to(dtype=torch.float)\n",
    "        logits: Float[Tensor, \"*batch_size n_tokens n_vocab\"] = self.model(inp)\n",
    "        log_probs = logits.log_softmax(-1)\n",
    "        correct_log_probs = log_probs[..., :-1,:].gather(\n",
    "                dim=-1, index=batch[..., 1:].unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "        loss = log_probs.neg()\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def train(\n",
    "        self, batches: list[Float[Tensor, \"n_tokens\"]]\n",
    "    ):\n",
    "        # wandb.init()\n",
    "        batches = [\n",
    "            torch.stack(batches[i:i+self.batch_size], dim=0)\n",
    "            for i in range(0, len(batches), self.batch_size)\n",
    "        ]\n",
    "        optimizer = self.optim_choice(self.model.parameters(), lr=self.lr) \n",
    "        for i, batch in enumerate(batches):\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.getLoss(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 0: print(f\"{loss=}\")\n",
    "            # wandb.log({\n",
    "            #     \"loss\": loss\n",
    "            # })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "inp.shape=torch.Size([8, 16])\n",
      "tensor([15,  9,  8, 10,  6, 15,  6])\n",
      "tensor([[ -3.0457,  -3.8715,  -3.3720,  -1.8847,  -5.2333,  -3.0917,  -2.9264,\n",
      "          -3.0351,  -3.3404,  -2.1282,  -2.9633,  -2.8837,  -2.1623,  -2.3566,\n",
      "          -2.2443,  -4.1902],\n",
      "        [ -4.0541,  -4.2653,  -4.5126,  -1.3380,  -6.9582,  -4.0187,  -2.7762,\n",
      "          -3.3060,  -3.5709,  -2.0323,  -3.8455,  -3.1923,  -2.4218,  -1.9716,\n",
      "          -2.1190,  -4.8727],\n",
      "        [ -3.9446,  -4.3051,  -2.1538,  -3.4117,  -4.8789,  -1.6333,  -3.1526,\n",
      "          -4.5522,  -3.9608,  -3.4527,  -2.1557,  -3.0251,  -1.4247,  -3.1890,\n",
      "          -2.9318,  -4.4430],\n",
      "        [ -4.1827,  -6.4136,  -3.0013, -11.6580,  -0.1088,  -3.8150,  -9.4205,\n",
      "          -7.2648,  -7.6408,  -9.2219,  -4.7358,  -7.2731,  -7.6362, -10.7488,\n",
      "          -9.4079,  -5.7923],\n",
      "        [ -3.9642,  -6.6282,  -3.3176, -11.6749,  -0.0875,  -4.3227,  -9.6471,\n",
      "          -7.2775,  -7.6907,  -9.3055,  -4.8339,  -7.4193,  -7.9271, -10.8712,\n",
      "          -9.5650,  -5.6110],\n",
      "        [ -2.1128,  -6.2763,  -2.9109,  -9.6251,  -0.2609,  -4.0518,  -8.8970,\n",
      "          -5.7438,  -6.7852,  -7.3633,  -3.8872,  -6.3752,  -6.7870,  -9.4670,\n",
      "          -8.0299,  -5.0980],\n",
      "        [ -1.5136,  -4.7530,  -2.3831,  -7.0257,  -0.6208,  -3.1133,  -6.8682,\n",
      "          -3.9655,  -5.2494,  -5.1430,  -3.2812,  -4.8810,  -5.2631,  -7.1309,\n",
      "          -5.9023,  -4.4960]], grad_fn=<SliceBackward0>)\n",
      "tensor([-4.1902, -2.0323, -3.9608, -4.7358, -9.6471, -5.0980, -6.8682],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "TransformerTrainer(Transformer()).getLoss(torch.randint(0, 16, (8, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  0,  3, 12, 13,  3,  2,  2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(6.8083, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(2.7811, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(2.7604, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(2.4596, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(2.4979, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(2.4074, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(2.1470, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(2.3901, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(2.2935, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(2.0583, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(2.1675, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(2.1607, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(1.9552, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(1.9228, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(1.9755, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(1.9383, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(1.9066, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(1.9366, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(1.9347, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(1.9064, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(1.9182, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(1.9307, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(1.9037, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(1.9105, grad_fn=<MeanBackward0>)\n",
      "loss=tensor(1.9238, grad_fn=<MeanBackward0>)\n",
      "after\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4,  5,  4,  4, 13])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  5,  6,  7,  8,  6,  6, 14])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "debugModel = Transformer(num_layers=5, d_model=8, n_vocab=16, n_head=4)\n",
    "# debugModel = Transformer()\n",
    "for _ in range(1):\n",
    "    display(debugModel.inference([0,1,2,3,4]))\n",
    "    pass\n",
    "debugBatch = [torch.tensor([(x + i) % 15 for i in range(8)], dtype=torch.int64) for x in range(9990)]\n",
    "# display(debugBatch[:10])\n",
    "trainer = TransformerTrainer(debugModel, epochs=1, lr=1e-3)\n",
    "trainer.train(debugBatch)\n",
    "print(\"after\\n\")\n",
    "for _ in range(1):\n",
    "    display(debugModel.inference([0,1,2,3,4]))\n",
    "    display(debugModel.inference([3,4,5,6,7]))\n",
    "# debugBatch\n",
    "# debugModel.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, t in debugModel.named_parameters():\n",
    "#     display(f\"{name}, {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
